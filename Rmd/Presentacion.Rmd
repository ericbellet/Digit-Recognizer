---
title: "Reconocedor de Dígitos Usando SVM"
author: "Eric Bellet, Deyban Pérez, Leonardo Santella"
date: "14 de mayo de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####Abstract
El problema de reconocimiento de imágenes ha sido ampliamente estuidiado por la comunidad científica de la computación, elaborar modelos que permitan el reconomiento de imágenes basándose en conocimiento previo es una de las aplicaciones más poderosas en cuanto al área de la **Inteligencia Artificial** se refiere, ya que su uso puede ir desde reconomiento de rostros, placas, etc. Que automaticen y agilicen el procesamiento y reconocimiento de patrones.

#Introducción
Ente informe contiene la solución elaborada por los integrantes mencionados previamente que cursan la materia de **Minería de Datos** de la **Universidad Central de Venezuela** dictada por el profesor **Fernando Crema** durante el período **02-20015** al problema de reconociento de imágenes colocado en el sitio web de  [kaggle](https://www.kaggle.com/) utilizando como algoritmo de clasificación **Máquina de Soporte Vectorial** (**SVM**) para clasificar entre un conjunto de imágenes que contienen ejemplos de números escritos a mano que varían en el rango de [0,9].

El objetivo es crear modelos con los diferentes tipos de kernel provistos por el paquete que proporciona el lenguaje **R** en su paquete **e1071** para realizar SVM, analizarlos, compararlos y generar una aplicación interactiva que sirva como desmostración del trabajo realizado.

#Máquina de Soporte Vectorial (SVM)

Es una técnica de aprendizaje supervisado que funciona tanto para clasificación como para regresión. Este método consiste en la búsqueda de hiperplanos que **maximicen** la distancia de separación entre las diferentes clases presentes mediante la búsqueda de vectores de soporte, la idea es maximizar la distancia de proyección entre la línea fontera y los vectores de soporte presentes en el conjutno de datos..  

#Análisis del Problema
Se tienen imágenes de tamaño **28X28** pixeles, lo que nos da un total de **784 píxeles** que son representado en nuestro **dataset de entrenamiento** por 784 columnas, donde cada columna corresponde a un píxel en el rango **[0,255]** que indica la claridad o la oscuridad del píxel.

El objetivo principal es usar el algoritmo de aprendizaje supervidado de **Máquina de Soporte Vectorial** para crear modelos (uno por cada kernel provisto por el paquete mencionado anteriormente) y luego compararlos mediante la técnica de evaluación de matriz de evaluación y así evaluar su tasa de acierto y fallo.

El problema lo dividimos en tres (3) secciones

* **Preprocesamiento de Parámetros**: dependiendo del kernel seleccionado, se necesitan ajustar ninguno, uno o varios parámetros que ayuden a ajustar el modelo hacia una mejor solución, en esta sección se explicarán las actividades realizadas y los análisis realizados que se usaron para llevar a cabo la preselección de parámetros para entrenar el modelo.

* **Entrenamiento de los Modelos y Evaluación de Modelos**: se describirá el entrenamiento de los modelos tomando en cuenta los parámetros obtenidos del paso anterior y luego la correspondiente evaluación y comparación entre estos.

* **Aplicación Interactiva Shiny**: con los modelos entrenados, se creó una aplicación que para una entrada desconocida aplica los modelos entrenados y luego mediante un esquema de votación entre la salida de los modelos se emite el juicio final.

Dicho esto, a continuación empezaremos a describir las distantas secciones mencionadas previamente.


#Definición de Funciones a Utilizar

A continuación se definen las funciones a utilizar a lo largo del documentos:

1. **install**: función que instala un paquete pasado como parámetro si este no se encuentra instalado. 

```{r}
install = function(pkg)
{
  # If is is installed does not install packages
  if (!require(pkg, character.only = TRUE))
  {
    install.packages(pkg)
    if (!require(pkg, character.only = TRUE))
      stop(paste("load failure:", pkg))
  }
}
```

2. **plotDigit**: función que toma como parámetros la fila y un dataset y genera la gráfica del número en un formato de 28x28 píxeles. 

```{r}
plotDigit = function(row, dataSet)
{
  image.data = matrix(data.matrix(dataSet[row, 1:ncol(dataSet)]), 28, 28)
  image.data = t(image.data)
  image.data = t(image.data)[ ,nrow(image.data):1]
  image(x = image.data, col = c(0,1))
}
```

3. **sumDiag**: función que toma una matriz de confusión y retorna la suma de la diagonal de dicha matriz de confusión.

```{r}
sumDiag = function(confusionMatrix)
{
  returnValue = 0
  
  for (i in (1:nrow(confusionMatrix)))
    returnValue = returnValue + confusionMatrix[i,i]
  
  return(returnValue)
}
```

4. **testModel**: función que recibe cómo parámetros un número de fila y un dataset, grafica dicho número y utiliza los diferentes modelos generados para predecir el ejemplo pasado como parámetro. Al final entre los cuatro (4) modelos se hace un esquema de votación y la respuesta que haya tenido mayor cantidad de votos es la salida que se muestra como solución final.

```{r}
testModel = function(row, dataSet)
{
  plotDigit(row, dataSet)
  write(sprintf("Modelo lineal: %s",
                as.matrix(predict(model_linear,
                                  dataSet[row,]))[1][1]),
        stdout())
  
  linear <- as.matrix(predict(model_linear, dataSet[row,]))[1][1]
  
  write(sprintf("Modelo polinomico: %s",
                as.matrix(predict(model_polynomial,
                                  dataSet[row,]))[1][1]),
        stdout())
  
  polynomial <- as.matrix(predict(model_polynomial,
                                  dataSet[row,]))[1][1]
  
  write(sprintf("Modelo radial: %s",
                as.matrix(predict(model_radial,
                                  dataSet[row,]))[1][1]),
        stdout())
  
  radial <- as.matrix(predict(model_radial, dataSet[row,]))[1][1]
  
  write(sprintf("Modelo sigmoid: %s",
                as.matrix(predict(model_sigmoid,
                                  dataSet[row,]))[1][1]),
        stdout())
  
  sigmoid <- as.matrix(predict(model_sigmoid, dataSet[row,]))[1][1]
  
  write(sprintf("Segun el esquema de votacion el numero es: %s",
                names(sort(summary(as.factor(c(linear, polynomial,
                                               radial, sigmoid))),
                           decreasing=T)[1])), stdout())
}
```

#Instalando y Cargando los Paquetes a Utilizar

El paquete **e1071** es el único paquete externo que se utilizará.

```{r}
install("e1071")
library(e1071)
```


#Preprocesamiento de Parámetros

Comencemos por cargar nuestro dataset de entrenamiento

```{r}
train = read.csv("../data/train.csv")
```

Veamos las dimensiones del dataset de entrenamiento:

```{r}
nrow(train)
ncol(train)
```

Podemos ver que hay 45000 ejemplos de entrenamiento, y 785 columnas de la siguiente forma:

```{r}
train[1,1:4]
```

Donde la primera columna se refiere a la etiqueta de la clase y el restante a los 784 píxeles con valores entre [0, 255] que representan la claridad y oscuridad de cada píxel.

El asunto es el siguiente, tenemos las siguientes fórmulas de los diferentes tipos de kernel:

* **Linear**: u' x v

* **Polynomial**: (gamma x u' x v + coef0) ^ degree

* **Radial**: exp(-gamma x |u - v| ^ 2)

* **Sigmoid**: tanh(gamma x u' x v + coef0)

Donde **u** y **v** son vectores que corresponden a los ejemplos y al vector **theta** que se usa como margen de separación pra la división de las clases  respectivamente, al multiplicar estos dos vectores se haya la proyección del vector **v** sobre el vector **u**. Y **gamma**, **coef0**, **degree**. Corresponden parámettros que deben ir ajustándose para ir entrenar de la mejor manera a nuestro modelo.

La pregunta ahora es: ¿Cómo saber que parámetros seleccionar?

Las dimensiones del dataset son muy grandes y verlas de una manera gráfica no nos podrá ayudar de mucho. Debido a esto decidimos hacer uso de la siguiente función **tune.svm()** esta función provista por el paquete **e1071** corre el algoritmo de SVM con diferentes parámetros y retorna los mejores parámetros para posteriormente pasárselos al algoritmo y poder entrenar el modelo.

El kernel polinómico cuya ecuación es: **(gamma x u' x v + coef0) ^ degree** podemos ver que tiene tres (3) parámetros los cuales se deben ajustar a conveniencia. esto quiere decir que si le pasamos un rango por ejemplo de dos (2) valores a cada uno de ellos supongamos [1,2] para cada uno de ellos, la función deberá entrenar el modelo y hacer las pruebas para **gamma = 1**, **coef0 = 1**, **degree = 1**, luego para **gamma = 1**, **coef0 = 1**, **degree = 2** y así sucesivamente hasta llegar a: **gamma = 2**, **coef0 = 2**, **degree = 2**. En total se realizarán 6 combinaciones y luego se deberán comparar los distintos modelos y arrojar como salida los parámetros que hicieron que el modelo fuera mejor.

Teniendo en cuenta que nuestro conjunto de entrenamiento consta de 45000 ejemplos y que el orden del algoritmo SVM es O(n ^ 3) donde **n** es el tamaño del dataset de entrenamiento, podemos darnos cuenta de que el algoritmo con 45000 filas tardará un tiempo entrenando, ahora imagínense si debe hacerlo seis (6) veces sólo para precalcular los parámetros. A simple vista nos podemos dar cuenta de que podrían pasar varios días en el proceso.

Dicho esto para abordar esta problemática, la estrategia utilizada fue hacer un sampling para reducir el número de filas a mil (1000) y utilizar ese número para precalcular los parámetros y luego utilizar dichos parámetros para entrenar los modelos con una mayor cantidad de ejemplos.

```{r}
set.seed(22)
subIndex = sample(nrow(train), 1000, replace = F)
training.data = train[subIndex, ]
```

Convertimos la columna **label** a tipo **factor** porque si está está en tipo numérico el algoritmo entonces hará regresión.

```{r}
training.data$label = as.factor(training.data$label)
```

Y ahora precalcularemos los mejores parámetros para los diferentes modelos:

##Polinómico

```{r}
#parameters_polynomial = tune.svm(label ~ .,
#                                 data = training.data,
#                                 kernel = "polynomial",
#                                 scale = F,
#                                 gamma = 10^(-6:-1),
#                                 cost = 10^(-1:1),
#                                 degree = (2:4),
#                                 coef0 = (-1:1))

#parameters_polynomial$best.model
```

##Radial

```{r}
#parameters_radial = tune.svm(label ~ .,
#                             data = training.data,
#                             kernel = "radial",
#                             scale = F,
#                             gamma = 10^(-6:-1))

#parameters_radial$best.model
```

##Sigmoid

```{r}
#parameters_sigmoid = tune.svm(label ~ .,
#                             data = training.data,
#                              kernel = "sigmoid",
#                              scale = F,
#                              gamma = 10^(-6:-1),
#                              coef0 = (-1:1))

#parameters_sigmoid$best.model
```

Por último salvamos los mejores parámetros para poder utilizarlos a la hora de entrenar los modelos.

```{r}
#save(parameters_polynomial, file = "parameters/parameters_polynomial.rda")
#save(parameters_radial, file = "parameters/parameters_radial.rda")
#save(parameters_sigmoid, file = "parameters/parameters_sigmoid.rda")
```

Cabe destacar que para el modelo lineal, no hace falta ajustar ningún parámetro y por eso no se encuentra presente.